{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76c0001",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Reorganized + argparse wrapper for the provided script.\n",
    "\n",
    "Modes:\n",
    "  a) oard_only      : only use OARD (no HPO_db updates; no HPO_db diseases; no HPO_db pair frequencies)\n",
    "  b) oard_first     : prioritize OARD over HPO_db (HPO update + pair freq preference)\n",
    "  c) hpodb_first    : prioritize HPO_db over OARD\n",
    "  d) hpodb_only     : only use HPO_db (no OARD calls)\n",
    "\n",
    "freq_assignment (used when HPO_db is enabled in mode b/c/d):\n",
    "  mean, median, max,\n",
    "  extrinsic_ic, intrinsic_ic, ic (ssmpy.ssm.information_content),\n",
    "  assumption (fallback/else branch: 0.01)\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import ssmpy\n",
    "from scipy.stats import multivariate_normal, norm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "MVN_CACHE = {}\n",
    "# =========================================================\n",
    "# SSMPY SETUP\n",
    "# =========================================================\n",
    "def init_ssmpy(hp_db_sqlite: str) -> None:\n",
    "    ssmpy.ssm.mica = True\n",
    "    ssmpy.ssm.intrinsic = True\n",
    "    ssmpy.semantic_base(hp_db_sqlite)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# LOAD DATA\n",
    "# =========================================================\n",
    "def load_oard(url: str, dataset_id: str = \"2\"):\n",
    "    # HPO domain\n",
    "    params = {\"dataset_id\": dataset_id, \"domain_id\": \"phenotypes\"}\n",
    "    oard_data = pd.DataFrame(\n",
    "        requests.get(url + \"/frequencies/mostFrequency\", params=params, verify=False)\n",
    "        .json()[\"results\"]\n",
    "    )\n",
    "\n",
    "    hpo_oard = list(set(oard_data[\"concept_code\"]))\n",
    "    oard_hpo_freq_map = oard_data.set_index(\"concept_code\")[\"concept_frequency\"].to_dict()\n",
    "    oard_hpo_code2id = oard_data.set_index(\"concept_code\")[\"concept_id\"].to_dict()\n",
    "\n",
    "    # Disease domain\n",
    "    params = {\"dataset_id\": dataset_id, \"domain_id\": \"diseases\"}\n",
    "    domain_disease_df = pd.DataFrame(\n",
    "        requests.get(url + \"/frequencies/mostFrequency\", params=params, verify=False)\n",
    "        .json()[\"results\"]\n",
    "    )\n",
    "\n",
    "    oard_disease = list(set(domain_disease_df[\"concept_id\"]))\n",
    "\n",
    "    return {\n",
    "        \"oard_data\": oard_data,\n",
    "        \"hpo_oard\": hpo_oard,\n",
    "        \"oard_hpo_freq_map\": oard_hpo_freq_map,\n",
    "        \"oard_hpo_code2id\": oard_hpo_code2id,\n",
    "        \"domain_disease_df\": domain_disease_df,\n",
    "        \"oard_disease\": oard_disease\n",
    "    }\n",
    "\n",
    "\n",
    "def load_hpo_db(hpo_db_path: str):\n",
    "    hpo_db = pd.read_csv(hpo_db_path, sep=\"\\t\")\n",
    "    hpo_db = hpo_db.rename(\n",
    "        columns={\n",
    "            \"hpo_id\": \"concept_code_1\",\n",
    "            \"mondo_id\": \"concept_code_2\",\n",
    "            \"frequency\": \"concept_frequency\",\n",
    "        }\n",
    "    )\n",
    "    hpo_db[\"concept_id_1\"] = hpo_db[\"concept_code_1\"].apply(lambda x: int(x.replace(\"HP:\", \"9\")))\n",
    "    hpo_db[\"concept_id_2\"] = hpo_db[\"concept_code_2\"].apply(lambda x: int(x.replace(\"MONDO:\", \"8\")))\n",
    "\n",
    "    hpo_db_freq_lookup = dict(\n",
    "        zip(\n",
    "            zip(hpo_db[\"concept_code_1\"], hpo_db[\"concept_id_2\"]),\n",
    "            hpo_db[\"concept_frequency\"],\n",
    "        )\n",
    "    )\n",
    "    hpo_db_index = hpo_db.groupby(\"concept_code_1\")[\"concept_id_2\"].apply(list).to_dict()\n",
    "\n",
    "    return {\n",
    "        \"hpo_db\": hpo_db,\n",
    "        \"hpo_db_freq_lookup\": hpo_db_freq_lookup,\n",
    "        \"hpo_db_index\": hpo_db_index,\n",
    "    }\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# MARGINAL FREQ FROM HPO_DB\n",
    "# =========================================================\n",
    "def build_hpo_db_marginal_freq(hpo_db: pd.DataFrame, freq_assignment: str):\n",
    "    \"\"\"\n",
    "    Returns a function: get_freq(hpo_code) -> float\n",
    "    Computes marginal frequency only for requested HPOs (lazy evaluation).\n",
    "    \"\"\"\n",
    "\n",
    "    hpo_db_group = hpo_db.groupby(\"concept_code_1\")[\"concept_frequency\"]\n",
    "    cache = {}\n",
    "\n",
    "    def get_freq(hpo_code: str):\n",
    "\n",
    "        if hpo_code in cache:\n",
    "            return cache[hpo_code]\n",
    "\n",
    "        if freq_assignment == \"mean\":\n",
    "            val = float(hpo_db_group.mean().get(hpo_code, 0.01))\n",
    "\n",
    "        elif freq_assignment == \"median\":\n",
    "            val = float(hpo_db_group.median().get(hpo_code, 0.01))\n",
    "\n",
    "        elif freq_assignment == \"max\":\n",
    "            val = float(hpo_db_group.max().get(hpo_code, 0.01))\n",
    "\n",
    "        elif freq_assignment in {\"extrinsic_ic\", \"intrinsic_ic\", \"ic\"}:\n",
    "            hid = ssmpy.get_id(hpo_code.replace(\":\", \"_\"))\n",
    "            if hid != -1:\n",
    "                if freq_assignment == \"extrinsic_ic\":\n",
    "                    ic = ssmpy.ssm.information_content_extrinsic(hid)\n",
    "                elif freq_assignment == \"intrinsic_ic\":\n",
    "                    ic = ssmpy.ssm.information_content_intrinsic(hid)\n",
    "                else:\n",
    "                    ic = ssmpy.ssm.information_content(hid)\n",
    "                val = float(np.exp(-ic))\n",
    "            else:\n",
    "                val = 0.01\n",
    "\n",
    "        else:  # assumption\n",
    "            val = 0.01\n",
    "\n",
    "        cache[hpo_code] = val\n",
    "        return val\n",
    "\n",
    "    return get_freq\n",
    "def get_hpo_ic_weight(hpo_code, caches, ic_type=\"extrinsic_ic\"):\n",
    "\n",
    "    if not hasattr(caches, \"hpo_ic_cache\"):\n",
    "        caches.hpo_ic_cache = {}\n",
    "\n",
    "    key = (hpo_code, ic_type)\n",
    "    if key in caches.hpo_ic_cache:\n",
    "        return caches.hpo_ic_cache[key]\n",
    "\n",
    "    hid = ssmpy.get_id(hpo_code.replace(\":\", \"_\"))\n",
    "    if hid == -1:\n",
    "        w = 0.0\n",
    "    else:\n",
    "        if ic_type == \"extrinsic_ic\":\n",
    "            w = float(ssmpy.ssm.information_content_extrinsic(hid))\n",
    "        elif ic_type == \"intrinsic_ic\":\n",
    "            w = float(ssmpy.ssm.information_content_intrinsic(hid))\n",
    "        else:\n",
    "            w = float(ssmpy.ssm.information_content(hid))\n",
    "\n",
    "    caches.hpo_ic_cache[key] = w\n",
    "    return w\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# CACHES + SIMILARITY\n",
    "# =========================================================\n",
    "class Caches:\n",
    "    def __init__(self):\n",
    "        self.oard_hpo_api_cache = {}\n",
    "        self.hpo_freq_cache = {}\n",
    "        self.ssmpy_id_cache = {}\n",
    "\n",
    "    def get_ssmpy_id(self, h: str) -> int:\n",
    "        if h not in self.ssmpy_id_cache:\n",
    "            self.ssmpy_id_cache[h] = ssmpy.get_id(h)\n",
    "        return self.ssmpy_id_cache[h]\n",
    "\n",
    "\n",
    "def calc_sim(hpo1: str, hpo2: str, caches: Caches, method: str = \"Resnik\"):\n",
    "    e1 = caches.get_ssmpy_id(hpo1)\n",
    "    e2 = caches.get_ssmpy_id(hpo2)\n",
    "    if e1 == -1 or e2 == -1:\n",
    "        return 0\n",
    "    elif method == \"Resnik\":\n",
    "        return ssmpy.ssm_resnik(e1, e2)\n",
    "    elif method == \"Lin\":\n",
    "        return ssmpy.ssm_lin(e1, e2)\n",
    "    elif method == \"JC\":\n",
    "        return ssmpy.ssm_jiang_conrath(e1, e2)\n",
    "    elif method == \"IC\":\n",
    "        return ssmpy.ssm_lin(e1, e2) * (1 - 1 / (1 + ssmpy.ssm_resnik(e1, e2)))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# ODDS + UPDATE_HPO (mode-dependent priority)\n",
    "# =========================================================\n",
    "def calc_odd_oard_fast(disease_idx, hpos, hpos_freq, rho, map2freq, x_base, cov_mat, mvn=None):\n",
    "    hpos = [each.replace(\"_\", \":\") for each in hpos]\n",
    "    p = 10 ^ -6\n",
    "    odd_disease = p / (1 - p)\n",
    "\n",
    "    x_disease = x_base.copy()\n",
    "    x_noDisease = x_base.copy()\n",
    "\n",
    "    for i, hpo in enumerate(hpos):\n",
    "        p_yes = map2freq.get((hpo.replace(\":\", \"_\"), disease_idx), -1)\n",
    "        if p_yes != -1:\n",
    "            x_disease[i] = norm.ppf(p_yes)\n",
    "            p_no = (hpos_freq[i] - p_yes * p) / (1 - p)\n",
    "            if p_no < 0:\n",
    "                p_no = 0\n",
    "            x_noDisease[i] = norm.ppf(p_no)\n",
    "\n",
    "    if mvn is None:\n",
    "        joint_prob1 = multivariate_normal.cdf(x_disease, cov=cov_mat)\n",
    "        joint_prob2 = multivariate_normal.cdf(x_noDisease, cov=cov_mat)\n",
    "    else:\n",
    "        joint_prob1 = mvn.cdf(x_disease)\n",
    "        joint_prob2 = mvn.cdf(x_noDisease)\n",
    "\n",
    "\n",
    "    return odd_disease * joint_prob1 / joint_prob2\n",
    "\n",
    "\n",
    "def update_hpo(\n",
    "    hpos,\n",
    "    *,\n",
    "    method: str,\n",
    "    mode: str,\n",
    "    caches: Caches,\n",
    "    # OARD\n",
    "    hpo_oard=None,\n",
    "    oard_hpo_freq_map=None,\n",
    "    # HPO_db marginal\n",
    "    hpo_db_get_marginal_freq=None,\n",
    "    hpo_default_frequency: float = 0.01,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns: (new_hpos, hpo_freq)\n",
    "    Priority is controlled by mode:\n",
    "      - oard_only:     OARD -> map-to-nearest-OARD\n",
    "      - hpodb_only:    HPO_db -> else default\n",
    "      - oard_first:    OARD -> HPO_db -> map-to-nearest-OARD\n",
    "      - hpodb_first:   HPO_db -> OARD -> map-to-nearest-OARD\n",
    "    \"\"\"\n",
    "    new_hpos = []\n",
    "    hpo_freq = []\n",
    "\n",
    "    for hpo in tqdm(hpos, position=1,  dynamic_ncols=True, leave=True):\n",
    "        hpo_code = hpo.replace(\"_\", \":\")\n",
    "\n",
    "        if mode == \"oard_only\":\n",
    "            if hpo_code in (oard_hpo_freq_map or {}):\n",
    "                new_hpos.append(hpo)\n",
    "                hpo_freq.append(float(oard_hpo_freq_map[hpo_code]))\n",
    "                continue\n",
    "            # map to nearest OARD (Resnik)\n",
    "            if hpo not in caches.hpo_freq_cache:\n",
    "                sim_scores = [calc_sim(each.replace(\":\", \"_\"), hpo, caches, method) for each in (hpo_oard or [])]\n",
    "                index_max = int(np.argmax(sim_scores)) if len(sim_scores) else 0\n",
    "                mapped_hpo = hpo_oard[index_max].replace(\":\", \"_\")\n",
    "                caches.hpo_freq_cache[hpo] = mapped_hpo\n",
    "            else:\n",
    "                mapped_hpo = caches.hpo_freq_cache[hpo]\n",
    "\n",
    "            new_hpos.append(mapped_hpo)\n",
    "            mapped_code = mapped_hpo.replace(\"_\", \":\")\n",
    "            hpo_freq.append(float((oard_hpo_freq_map or {}).get(mapped_code, hpo_default_frequency)))\n",
    "            continue\n",
    "\n",
    "        if mode == \"hpodb_only\":\n",
    "            if hpo_db_get_marginal_freq is not None:\n",
    "                val = hpo_db_get_marginal_freq(hpo_code)\n",
    "                if val is not None:\n",
    "                    new_hpos.append(hpo)\n",
    "                    hpo_freq.append(float(val))\n",
    "                    continue\n",
    "            else:\n",
    "                new_hpos.append(hpo)\n",
    "                hpo_freq.append(float(hpo_default_frequency))\n",
    "            continue\n",
    "\n",
    "        if mode == \"oard_first\":\n",
    "            # 1) OARD\n",
    "            if hpo_code in (oard_hpo_freq_map or {}):\n",
    "                new_hpos.append(hpo)\n",
    "                hpo_freq.append(float(oard_hpo_freq_map[hpo_code]))\n",
    "                continue\n",
    "            # 2) HPO_db\n",
    "            if hpo_db_get_marginal_freq is not None:\n",
    "                val = hpo_db_get_marginal_freq(hpo_code)\n",
    "                if val is not None:\n",
    "                    new_hpos.append(hpo)\n",
    "                    hpo_freq.append(float(val))\n",
    "                    continue\n",
    "\n",
    "            # 3) map to nearest OARD\n",
    "            if hpo not in caches.hpo_freq_cache:\n",
    "                sim_scores = [calc_sim(each.replace(\":\", \"_\"), hpo, caches, method) for each in (hpo_oard or [])]\n",
    "                index_max = int(np.argmax(sim_scores)) if len(sim_scores) else 0\n",
    "                mapped_hpo = hpo_oard[index_max].replace(\":\", \"_\")\n",
    "                caches.hpo_freq_cache[hpo] = mapped_hpo\n",
    "            else:\n",
    "                mapped_hpo = caches.hpo_freq_cache[hpo]\n",
    "\n",
    "            new_hpos.append(mapped_hpo)\n",
    "            mapped_code = mapped_hpo.replace(\"_\", \":\")\n",
    "            hpo_freq.append(float((oard_hpo_freq_map or {}).get(mapped_code, hpo_default_frequency)))\n",
    "            continue\n",
    "\n",
    "        if mode == \"hpodb_first\":\n",
    "            # 1) HPO_db\n",
    "            if hpo_db_get_marginal_freq is not None:\n",
    "                val = hpo_db_get_marginal_freq(hpo_code)\n",
    "                if val is not None:\n",
    "                    new_hpos.append(hpo)\n",
    "                    hpo_freq.append(float(val))\n",
    "                    continue\n",
    "            # 2) OARD\n",
    "            if hpo_code in (oard_hpo_freq_map or {}):\n",
    "                new_hpos.append(hpo)\n",
    "                hpo_freq.append(float(oard_hpo_freq_map[hpo_code]))\n",
    "                continue\n",
    "            # 3) map to nearest OARD\n",
    "            if hpo not in caches.hpo_freq_cache:\n",
    "                sim_scores = [calc_sim(each.replace(\":\", \"_\"), hpo, caches, method) for each in (hpo_oard or [])]\n",
    "                index_max = int(np.argmax(sim_scores)) if len(sim_scores) else 0\n",
    "                mapped_hpo = hpo_oard[index_max].replace(\":\", \"_\")\n",
    "                caches.hpo_freq_cache[hpo] = mapped_hpo\n",
    "            else:\n",
    "                mapped_hpo = caches.hpo_freq_cache[hpo]\n",
    "\n",
    "            new_hpos.append(mapped_hpo)\n",
    "            mapped_code = mapped_hpo.replace(\"_\", \":\")\n",
    "            hpo_freq.append(float((oard_hpo_freq_map or {}).get(mapped_code, hpo_default_frequency)))\n",
    "            continue\n",
    "\n",
    "        # Fallback (should not happen)\n",
    "        new_hpos.append(hpo)\n",
    "        hpo_freq.append(float(hpo_default_frequency))\n",
    "\n",
    "    return new_hpos, hpo_freq\n",
    "# =========================================================\n",
    "# ADAPTIVE BATCH EFFECT REMOVAL (PhenoSS style)\n",
    "# =========================================================\n",
    "\n",
    "DEPTH_CACHE = {}\n",
    "\n",
    "def get_depth_proxy(h, caches: Caches):\n",
    "    \"\"\"\n",
    "    Depth proxy using intrinsic IC.\n",
    "    Higher IC ≈ deeper / more specific.\n",
    "    \"\"\"\n",
    "    if h in DEPTH_CACHE:\n",
    "        return DEPTH_CACHE[h]\n",
    "\n",
    "    eid = caches.get_ssmpy_id(h)\n",
    "    if eid == -1:\n",
    "        val = 0.0\n",
    "    else:\n",
    "        try:\n",
    "            val = float(ssmpy.ssm.information_content_intrinsic(eid))\n",
    "        except:\n",
    "            val = 0.0\n",
    "\n",
    "    DEPTH_CACHE[h] = val\n",
    "    return val\n",
    "\n",
    "\n",
    "def compute_avg_depth(hpo_patients, caches: Caches):\n",
    "    vals = []\n",
    "    for hpos in hpo_patients.values():\n",
    "        for h in hpos:\n",
    "            vals.append(get_depth_proxy(h, caches))\n",
    "    return float(np.mean(vals)) if len(vals) else 0.0\n",
    "\n",
    "\n",
    "def adaptive_batch_correction(\n",
    "    hpo_patients: dict,\n",
    "    freq_patients: dict,\n",
    "    caches: Caches,\n",
    "    min_terms: int = 1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Implements manuscript logic:\n",
    "      - search threshold T over depth values\n",
    "      - remove terms depth < T\n",
    "      - ensure each patient keeps >= min_terms\n",
    "      - choose largest valid T\n",
    "    \"\"\"\n",
    "\n",
    "    # collect candidate thresholds\n",
    "    all_depths = sorted({\n",
    "        get_depth_proxy(h, caches)\n",
    "        for v in hpo_patients.values()\n",
    "        for h in v\n",
    "    })\n",
    "\n",
    "    if not all_depths:\n",
    "        return hpo_patients, freq_patients, 0.0\n",
    "\n",
    "    original_avg = compute_avg_depth(hpo_patients, caches)\n",
    "    best_T = 0.0\n",
    "    best_hpo = hpo_patients\n",
    "    best_freq = freq_patients\n",
    "\n",
    "    for T in all_depths:\n",
    "\n",
    "        new_hpo = {}\n",
    "        new_freq = {}\n",
    "        valid = True\n",
    "\n",
    "        for pid, hpos in hpo_patients.items():\n",
    "            freqs = freq_patients[pid]\n",
    "\n",
    "            pairs = list(zip(hpos, freqs))\n",
    "\n",
    "            kept = [\n",
    "                (h, f) for (h, f) in pairs\n",
    "                if get_depth_proxy(h, caches) >= T\n",
    "            ]\n",
    "\n",
    "            # safeguard\n",
    "            if len(kept) < min_terms:\n",
    "                # keep most specific\n",
    "                pairs_sorted = sorted(\n",
    "                    pairs,\n",
    "                    key=lambda x: get_depth_proxy(x[0], caches),\n",
    "                    reverse=True\n",
    "                )\n",
    "                kept = pairs_sorted[:max(min_terms, 1)]\n",
    "\n",
    "            if len(kept) == 0:\n",
    "                valid = False\n",
    "                break\n",
    "\n",
    "            new_hpo[pid] = [h for h, _ in kept]\n",
    "            new_freq[pid] = [f for _, f in kept]\n",
    "\n",
    "        if not valid:\n",
    "            continue\n",
    "\n",
    "        new_avg = compute_avg_depth(new_hpo, caches)\n",
    "\n",
    "        # manuscript rule: increase precision\n",
    "        if new_avg > original_avg:\n",
    "            best_T = T\n",
    "            best_hpo = new_hpo\n",
    "            best_freq = new_freq\n",
    "\n",
    "    return best_hpo, best_freq, best_T\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# PATIENT INPUT\n",
    "# =========================================================\n",
    "def load_patients(inputfile: str, *, update_fn):\n",
    "    hpo_patients = {}\n",
    "    freq_patients = {}\n",
    "    with open(inputfile) as infile:\n",
    "        for line in tqdm(infile, position=0, desc = 'Entering patient info'):\n",
    "            parts = line.strip().split()\n",
    "            pid = parts[0]\n",
    "            hpos = parts[1].split(\";\")[:-1]\n",
    "            new_hpos, new_freq = update_fn(hpos)\n",
    "            hpo_patients[pid] = new_hpos\n",
    "            freq_patients[pid] = new_freq\n",
    "    return hpo_patients, freq_patients\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# BUILD disease2freq + hpo2diseases (mode-dependent)\n",
    "# =========================================================\n",
    "def build_disease_maps(\n",
    "    *,\n",
    "    mode: str,\n",
    "    caches: Caches,\n",
    "    url: str,\n",
    "    dataset_id: str,\n",
    "    # patient data\n",
    "    hpo_patients: dict,\n",
    "    # OARD\n",
    "    oard_hpo_code2id=None,\n",
    "    # HPO_db\n",
    "    hpo_db_index=None,\n",
    "    hpo_db_freq_lookup=None,\n",
    "    # preference\n",
    "    prefer: str,  # \"oard\" or \"hpodb\"\n",
    "):\n",
    "    disease2freq = {}\n",
    "    hpo2diseases = defaultdict(set)\n",
    "\n",
    "    final_diseases = set()\n",
    "    all_patient_hpos = set([h for lst in hpo_patients.values() for h in lst])\n",
    "\n",
    "    for each_hpo in tqdm(all_patient_hpos, desc='Retrieving Frequency'):\n",
    "        hpo_key = each_hpo.replace(\":\", \"_\")\n",
    "        each_hpo_code = each_hpo.replace(\"_\", \":\")\n",
    "\n",
    "        # ---- collect candidate diseases ----\n",
    "        if mode in {\"hpodb_only\", \"hpodb_first\", \"oard_first\"}:\n",
    "            final_diseases.update((hpo_db_index or {}).get(each_hpo_code, []))\n",
    "\n",
    "        # ---- OARD pull for this HPO ----\n",
    "        oard_lookup = {}\n",
    "        oard_final_dict = set()\n",
    "        hpo_id = None\n",
    "\n",
    "        if mode in {\"oard_only\", \"oard_first\", \"hpodb_first\"}:\n",
    "            hpo_id = (oard_hpo_code2id or {}).get(each_hpo_code, None)\n",
    "            if hpo_id:\n",
    "                if hpo_id not in caches.oard_hpo_api_cache:\n",
    "                    params = {\"dataset_id\": dataset_id, \"concept_id\": hpo_id}\n",
    "                    result_df = pd.DataFrame(\n",
    "                        requests.get(url + \"/frequencies/mostFrequency\", params=params, verify=False)\n",
    "                        .json()[\"results\"]\n",
    "                    )\n",
    "                    caches.oard_hpo_api_cache[hpo_id] = result_df\n",
    "                else:\n",
    "                    result_df = caches.oard_hpo_api_cache[hpo_id]\n",
    "\n",
    "                if len(result_df) > 0:\n",
    "                    result_df = result_df[result_df[\"concept_id_2\"].astype(str).str.startswith(\"8\")]\n",
    "                    oard_lookup = dict(\n",
    "                        zip(\n",
    "                            zip(result_df[\"concept_id_1\"], result_df[\"concept_id_2\"]),\n",
    "                            result_df[\"concept_frequency\"],\n",
    "                        )\n",
    "                    )\n",
    "                    oard_final_dict = set([d for (_, d) in oard_lookup.keys()])\n",
    "\n",
    "        if mode in {\"oard_only\", \"oard_first\", \"hpodb_first\"}:\n",
    "            final_diseases.update(oard_final_dict)\n",
    "\n",
    "        # ---- fill disease2freq + hpo2diseases ----\n",
    "        for dis in final_diseases:\n",
    "            # preference for pair frequency\n",
    "            if mode == \"oard_only\":\n",
    "                val = oard_lookup.get((hpo_id, dis), -1) if hpo_id else -1\n",
    "\n",
    "            elif mode == \"hpodb_only\":\n",
    "                val = (hpo_db_freq_lookup or {}).get((each_hpo_code, dis), -1)\n",
    "\n",
    "            else:\n",
    "                # mixed\n",
    "                if prefer == \"oard\":\n",
    "                    val = oard_lookup.get((hpo_id, dis), -1) if hpo_id else -1\n",
    "                    if val == -1:\n",
    "                        val = (hpo_db_freq_lookup or {}).get((each_hpo_code, dis), -1)\n",
    "                else:\n",
    "                    val = (hpo_db_freq_lookup or {}).get((each_hpo_code, dis), -1)\n",
    "                    if val == -1:\n",
    "                        val = oard_lookup.get((hpo_id, dis), -1) if hpo_id else -1\n",
    "\n",
    "            disease2freq[(hpo_key, dis)] = val\n",
    "            hpo2diseases[hpo_key].add(dis)\n",
    "\n",
    "    return disease2freq, hpo2diseases\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# RANK PATIENTS\n",
    "# =========================================================\n",
    "def get_mvn_for_dimension(n, cov_mat):\n",
    "    if n not in MVN_CACHE:\n",
    "        MVN_CACHE[n] = multivariate_normal(mean=np.zeros(n), cov=cov_mat)\n",
    "    return MVN_CACHE[n]\n",
    "def computing_phenoss(hpo_patients, freq_patients, disease2freq, hpo2diseases, args, caches):\n",
    "    rank_patients = {}\n",
    "    for pid, patient_hpos in tqdm(hpo_patients.items(), position=0, desc = 'Computing the score'):\n",
    "        patient_freq = freq_patients[pid]\n",
    "\n",
    "        x_base = np.array([norm.ppf(p) for p in patient_freq])\n",
    "        n = len(patient_hpos)\n",
    "\n",
    "        cov_mat = 0.01 * np.ones([n, n])\n",
    "        np.fill_diagonal(cov_mat, 1)\n",
    "\n",
    "        mvn = get_mvn_for_dimension(n, cov_mat)\n",
    "        # ---------- IC-weighted candidate filtering ----------\n",
    "        if (args.mode == 'oard_only') or (args.limit==0):\n",
    "            candidate_diseases = set()\n",
    "            for h in patient_hpos:\n",
    "                candidate_diseases.update(hpo2diseases.get(h, []))\n",
    "        else:\n",
    "            if len(patient_hpos) <= 1:\n",
    "                candidate_diseases = set()\n",
    "                for h in patient_hpos:\n",
    "                    candidate_diseases.update(hpo2diseases.get(h, []))\n",
    "            else:\n",
    "\n",
    "                disease_score = defaultdict(float)\n",
    "                disease_count = defaultdict(int)\n",
    "\n",
    "                for h in patient_hpos:\n",
    "                    h_code = h.replace(\"_\", \":\")\n",
    "                    w = get_hpo_ic_weight(h_code, caches)\n",
    "\n",
    "                    for d in hpo2diseases.get(h, []):\n",
    "                        disease_score[d] += w\n",
    "                        disease_count[d] += 1\n",
    "\n",
    "                # sort by IC-weighted score\n",
    "                sorted_items = sorted(\n",
    "                    disease_score.items(),\n",
    "                    key=lambda x: (x[1], disease_count[x[0]]),\n",
    "                    reverse=True\n",
    "                )\n",
    "\n",
    "                TARGET = args.limit\n",
    "                HARD_CAP = min(args.limit*2, 5000)\n",
    "\n",
    "                if len(sorted_items) <= TARGET:\n",
    "                    candidate_diseases = [d for d, _ in sorted_items]\n",
    "                else:\n",
    "                    cutoff_score = sorted_items[TARGET-1][1]\n",
    "\n",
    "                    candidate_diseases = [\n",
    "                        d for d, s in sorted_items\n",
    "                        if s >= cutoff_score\n",
    "                    ]\n",
    "\n",
    "                    if len(candidate_diseases) > HARD_CAP:\n",
    "                        candidate_diseases = [d for d, _ in sorted_items[:HARD_CAP]]\n",
    "        scores = {}\n",
    "        for dis in candidate_diseases:\n",
    "            scores[dis] = calc_odd_oard_fast(\n",
    "                dis,\n",
    "                patient_hpos,\n",
    "                patient_freq,\n",
    "                0.01,\n",
    "                disease2freq,\n",
    "                x_base,\n",
    "                cov_mat,\n",
    "                mvn=mvn\n",
    "            )\n",
    "\n",
    "        rank_patients[pid] = scores\n",
    "        print(\"finished patient:\", pid)\n",
    "\n",
    "    return rank_patients\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# OUTPUT\n",
    "# =========================================================\n",
    "def build_output_df(rank_patients, hpo_db: pd.DataFrame, gene_conversion: bool):\n",
    "    mondo2gene = (\n",
    "        hpo_db.groupby(\"concept_code_2\")[\"gene_symbol\"]\n",
    "        .apply(lambda x: list(set(x.dropna())))\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    records = []\n",
    "    for pid, dis2score in tqdm(rank_patients.items(), desc = 'Finalizing output'):\n",
    "        for dis, score in dis2score.items():\n",
    "            mondo_code = \"MONDO:\" + str(dis)[1:]\n",
    "            records.append(\n",
    "                {\n",
    "                    \"patient_id\": pid,\n",
    "                    \"disease_mondo\": mondo_code,\n",
    "                    \"gene\": mondo2gene.get(mondo_code, np.nan),  # list\n",
    "                    \"disease_id\": dis,\n",
    "                    \"score\": score,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    df_out = pd.DataFrame(records)\n",
    "    if gene_conversion:\n",
    "        df_out = df_out.explode(\"gene\", ignore_index=True)\n",
    "\n",
    "        # remove NaN genes\n",
    "        df_out = df_out[df_out[\"gene\"].notna()].copy()\n",
    "\n",
    "    # sort for consistency\n",
    "    df_out.sort_values([\"patient_id\", \"score\"], ascending=[True, True], inplace=True, ignore_index=True)\n",
    "    df_out[\"score\"] = pd.to_numeric(df_out[\"score\"], errors=\"coerce\")\n",
    "\n",
    "    # rerank per patient (same score -> same rank)\n",
    "    df_out[\"rank\"] = (\n",
    "        df_out.groupby(\"patient_id\")[\"score\"].rank(method=\"min\", ascending=True).astype(int)\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    df_out[\"sample_size\"] = len(df_out)\n",
    "    return df_out\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# MAIN\n",
    "# =========================================================\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser(description=\"PhenoSS ranking runner (reorganized).\")\n",
    "\n",
    "    p.add_argument(\"--inputfile\", required=True, type=str, help=\"Path to patient input file.\")\n",
    "    p.add_argument(\"--outputfile\", required=True, type=str, help=\"Path to output TSV/CSV.\")\n",
    "\n",
    "    p.add_argument(\n",
    "        \"--mode\",\n",
    "        required=True,\n",
    "        choices=[\"oard_only\", \"oard_first\", \"hpodb_first\", \"hpodb_only\"],\n",
    "        help=(\n",
    "            \"Evaluation mode: \"\n",
    "            \"oard_only (a), oard_first (b), hpodb_first (c), hpodb_only (d).\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    p.add_argument(\n",
    "        \"--freq_assignment\",\n",
    "        default=\"extrinsic_ic\",\n",
    "        choices=[\"mean\", \"median\", \"max\", \"extrinsic_ic\", \"intrinsic_ic\", \"ic\", \"assumption\"],\n",
    "        help=\"How to compute marginal HPO frequencies from HPO_db when HPO_db is enabled.\",\n",
    "    )\n",
    "\n",
    "    p.add_argument(\"--method\", default=\"Resnik\", type=str, help=\"Similarity method for mapping to OARD terms.\")\n",
    "    p.add_argument(\"--hp_db_sqlite\", default=\"hp.db\", type=str, help=\"Path to hp.db for ssmpy.\")\n",
    "    p.add_argument(\"--hpo_db_path\", default=\"hpo_frequency.csv\", type=str, help=\"Path to hpo_frequency.csv (tab-separated).\")\n",
    "    p.add_argument(\"--gene_conversion\", action=\"store_true\", help=\"Convert Mondo IDs to Genes\")\n",
    "    p.add_argument(\"--hpo_removal\", action=\"store_true\", help=\"Enable adaptive batch effect removal (PhenoSS style)\")\n",
    "    p.add_argument(\"--limit\", default=0, type=int, help=\"Limit the candidate space to N (help faster searching)\")\n",
    "\n",
    "    p.add_argument(\"--url\", default=\"https://rare.cohd.io/api\", type=str, help=\"OARD API base url.\")\n",
    "    p.add_argument(\"--dataset_id\", default=\"2\", type=str, help=\"OARD dataset_id (string).\")\n",
    "\n",
    "    # Optional debug extraction like your last lines\n",
    "    p.add_argument(\"--gene_of_interest\", default=None, type=str, help=\"If set, save rows for this gene.\")\n",
    "    p.add_argument(\"--gene_outfile\", default=None, type=str, help=\"If set, write the gene subset to this path.\")\n",
    "\n",
    "    return p.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39df2d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(\n",
    "    inputfile=\"/home/nguyenqm/projects/github/LLM-Gene-Prioritization/data/input/PhenoSS_input/sample2\",\n",
    "    outputfile=\"/home/nguyenqm/projects/github/PhenoSS/doc/paper_data/sample2.tsv\",\n",
    "\n",
    "    mode=\"hpodb_first\",        # \"oard_only\", \"oard_first\", \"hpodb_first\", \"hpodb_only\"\n",
    "    freq_assignment=\"mean\", #\"mean\", \"median\", \"max\", \"extrinsic_ic\", \"intrinsic_ic\", \"ic\", \"assumption\"\n",
    "\n",
    "    method=\"Resnik\", # Lin, JC, IC\n",
    "    hp_db_sqlite=\"hp.db\", # request from https://github.com/lasigeBioTM/DiShIn/tree/master\n",
    "    hpo_db_path=\"./databases/hpo_frequency.csv\", ## processed from HPO database\n",
    "    gene_conversion=True,\n",
    "    hpo_removal=True,\n",
    "    limit=0, # 0 if no limit and >0 for TARGET LIMIT, HARD CAP = limit*2\n",
    "    \n",
    "    url=\"https://rare.cohd.io/api\", # OARD\n",
    "    dataset_id=\"2\", # OARD - Columbia\n",
    "\n",
    "    gene_of_interest=\"LMNA\", # optional if having the golden label\n",
    "    gene_outfile=\"/home/nguyenqm/projects/github/PhenoSS/doc/paper_data/lmna.tsv\" # optional if having the golden label\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efd7fa01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing HPO Database\n",
      "Loading HPO Database\n",
      "Loading OARD Database\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing HPO Database\")\n",
    "init_ssmpy(args.hp_db_sqlite)\n",
    "\n",
    "caches = Caches()\n",
    "\n",
    "# Load HPO_db (always loaded because output step needs mondo2gene from it in your code)\n",
    "print(\"Loading HPO Database\")\n",
    "hpo_db_pack = load_hpo_db(args.hpo_db_path)\n",
    "hpo_db = hpo_db_pack[\"hpo_db\"]\n",
    "hpo_db_freq_lookup = hpo_db_pack[\"hpo_db_freq_lookup\"]\n",
    "hpo_db_index = hpo_db_pack[\"hpo_db_index\"]\n",
    "\n",
    "# Load OARD only if mode needs it\n",
    "print(\"Loading OARD Database\")\n",
    "if args.mode in {\"oard_only\", \"oard_first\", \"hpodb_first\"}:\n",
    "    oard_pack = load_oard(args.url, dataset_id=args.dataset_id)\n",
    "    hpo_oard = oard_pack[\"hpo_oard\"]\n",
    "    oard_hpo_freq_map = oard_pack[\"oard_hpo_freq_map\"]\n",
    "    oard_hpo_code2id = oard_pack[\"oard_hpo_code2id\"]\n",
    "else:\n",
    "    hpo_oard = []\n",
    "    oard_hpo_freq_map = {}\n",
    "    oard_hpo_code2id = {}\n",
    "\n",
    "# Build HPO_db marginal freq only if mode uses HPO_db during update_hpo\n",
    "if args.mode in {\"hpodb_only\", \"hpodb_first\", \"oard_first\"}:\n",
    "    hpo_db_get_marginal_freq = build_hpo_db_marginal_freq(hpo_db, args.freq_assignment)\n",
    "else:\n",
    "    hpo_db_get_marginal_freq = {}\n",
    "\n",
    "hpo_default_frequency = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f17be569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Patient Information\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entering patient info: 0it [00:00, ?it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 162.44it/s]\n",
      "\n",
      "100%|██████████| 7/7 [00:00<00:00, 392.92it/s]\n",
      "\n",
      "100%|██████████| 7/7 [00:00<00:00, 394.18it/s]\n",
      "\n",
      "100%|██████████| 7/7 [00:00<00:00, 471.74it/s]\n",
      "Entering patient info: 4it [00:00, 35.80it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 780.29it/s]\n",
      "Entering patient info: 5it [00:00, 39.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# update_hpo wrapper with mode + resources\n",
    "def _update_fn(hpos):\n",
    "    return update_hpo(\n",
    "        hpos,\n",
    "        method=args.method,\n",
    "        mode=args.mode,\n",
    "        caches=caches,\n",
    "        hpo_oard=hpo_oard,\n",
    "        oard_hpo_freq_map=oard_hpo_freq_map,\n",
    "        hpo_db_get_marginal_freq=hpo_db_get_marginal_freq,\n",
    "        hpo_default_frequency=hpo_default_frequency,\n",
    "    )\n",
    "\n",
    "# Load patients\n",
    "print(\"Loading Patient Information\")\n",
    "hpo_patients, freq_patients = load_patients(args.inputfile, update_fn=_update_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061da994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# Batch effect removal (AFTER update_hpo)\n",
    "# -------------------------------------------------\n",
    "if args.hpo_removal:\n",
    "    print(\"Applying adaptive batch correction\")\n",
    "\n",
    "    hpo_patients, freq_patients, T = adaptive_batch_correction(\n",
    "        hpo_patients,\n",
    "        freq_patients,\n",
    "        caches,\n",
    "        min_terms=1,\n",
    "    )\n",
    "\n",
    "    print(f\"Batch correction threshold (depth proxy) = {T}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8b49e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building HPO-Disease Mapping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving Frequency: 100%|██████████| 27/27 [00:11<00:00,  2.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# disease2freq / hpo2diseases\n",
    "if args.mode == \"oard_first\":\n",
    "    prefer = \"oard\"\n",
    "elif args.mode == \"hpodb_first\" or args.mode == \"hpodb_only\":\n",
    "    prefer = \"hpodb\"\n",
    "else:\n",
    "    prefer = \"oard\"\n",
    "\n",
    "print(\"Building HPO-Disease Mapping\")\n",
    "disease2freq, hpo2diseases = build_disease_maps(\n",
    "    mode=args.mode,\n",
    "    caches=caches,\n",
    "    url=args.url,\n",
    "    dataset_id=args.dataset_id,\n",
    "    hpo_patients=hpo_patients,\n",
    "    oard_hpo_code2id=oard_hpo_code2id,\n",
    "    hpo_db_index=hpo_db_index,\n",
    "    hpo_db_freq_lookup=hpo_db_freq_lookup,\n",
    "    prefer=prefer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8c3014f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PhenoSS scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing the score:  20%|██        | 1/5 [00:13<00:55, 13.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished patient: PAT_101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Computing the score:  40%|████      | 2/5 [00:36<00:56, 18.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished patient: PAT_102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Computing the score:  60%|██████    | 3/5 [01:02<00:44, 22.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished patient: PAT_103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Computing the score:  80%|████████  | 4/5 [01:27<00:23, 23.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished patient: PAT_104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing the score: 100%|██████████| 5/5 [01:40<00:00, 20.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished patient: PAT_105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute PhenoSS scores\n",
    "print(\"Computing PhenoSS scores\")\n",
    "phenoss_computation = computing_phenoss(hpo_patients, freq_patients, disease2freq, hpo2diseases, args, caches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83e198ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finalizing output: 100%|██████████| 5/5 [00:00<00:00, 765.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>disease_mondo</th>\n",
       "      <th>gene</th>\n",
       "      <th>disease_id</th>\n",
       "      <th>score</th>\n",
       "      <th>rank</th>\n",
       "      <th>sample_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PAT_101</td>\n",
       "      <td>MONDO:0958005</td>\n",
       "      <td>ERI1</td>\n",
       "      <td>80958005</td>\n",
       "      <td>-1.042511</td>\n",
       "      <td>1</td>\n",
       "      <td>11162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PAT_101</td>\n",
       "      <td>MONDO:0060510</td>\n",
       "      <td>EED</td>\n",
       "      <td>80060510</td>\n",
       "      <td>-1.041114</td>\n",
       "      <td>2</td>\n",
       "      <td>11162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PAT_101</td>\n",
       "      <td>MONDO:0010466</td>\n",
       "      <td>PIGA</td>\n",
       "      <td>80010466</td>\n",
       "      <td>-1.035104</td>\n",
       "      <td>3</td>\n",
       "      <td>11162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PAT_101</td>\n",
       "      <td>MONDO:0010478</td>\n",
       "      <td>SLC35A2</td>\n",
       "      <td>80010478</td>\n",
       "      <td>-1.033342</td>\n",
       "      <td>4</td>\n",
       "      <td>11162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PAT_101</td>\n",
       "      <td>MONDO:0012029</td>\n",
       "      <td>CPAP</td>\n",
       "      <td>80012029</td>\n",
       "      <td>-1.032221</td>\n",
       "      <td>5</td>\n",
       "      <td>11162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11157</th>\n",
       "      <td>PAT_105</td>\n",
       "      <td>MONDO:0030294</td>\n",
       "      <td>LMOD1</td>\n",
       "      <td>80030294</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1563</td>\n",
       "      <td>11162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11158</th>\n",
       "      <td>PAT_105</td>\n",
       "      <td>MONDO:0011017</td>\n",
       "      <td>JUP</td>\n",
       "      <td>80011017</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1563</td>\n",
       "      <td>11162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11159</th>\n",
       "      <td>PAT_105</td>\n",
       "      <td>MONDO:0012554</td>\n",
       "      <td>ERCC1</td>\n",
       "      <td>80012554</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1563</td>\n",
       "      <td>11162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11160</th>\n",
       "      <td>PAT_105</td>\n",
       "      <td>MONDO:0013819</td>\n",
       "      <td>ARID1A</td>\n",
       "      <td>80013819</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1563</td>\n",
       "      <td>11162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11161</th>\n",
       "      <td>PAT_105</td>\n",
       "      <td>MONDO:0014893</td>\n",
       "      <td>CSNK2A1</td>\n",
       "      <td>80014893</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1563</td>\n",
       "      <td>11162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11162 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      patient_id  disease_mondo     gene  disease_id     score  rank  \\\n",
       "0        PAT_101  MONDO:0958005     ERI1    80958005 -1.042511     1   \n",
       "1        PAT_101  MONDO:0060510      EED    80060510 -1.041114     2   \n",
       "2        PAT_101  MONDO:0010466     PIGA    80010466 -1.035104     3   \n",
       "3        PAT_101  MONDO:0010478  SLC35A2    80010478 -1.033342     4   \n",
       "4        PAT_101  MONDO:0012029     CPAP    80012029 -1.032221     5   \n",
       "...          ...            ...      ...         ...       ...   ...   \n",
       "11157    PAT_105  MONDO:0030294    LMOD1    80030294 -0.000000  1563   \n",
       "11158    PAT_105  MONDO:0011017      JUP    80011017 -0.000000  1563   \n",
       "11159    PAT_105  MONDO:0012554    ERCC1    80012554 -0.000000  1563   \n",
       "11160    PAT_105  MONDO:0013819   ARID1A    80013819 -0.000000  1563   \n",
       "11161    PAT_105  MONDO:0014893  CSNK2A1    80014893 -0.000000  1563   \n",
       "\n",
       "       sample_size  \n",
       "0            11162  \n",
       "1            11162  \n",
       "2            11162  \n",
       "3            11162  \n",
       "4            11162  \n",
       "...            ...  \n",
       "11157        11162  \n",
       "11158        11162  \n",
       "11159        11162  \n",
       "11160        11162  \n",
       "11161        11162  \n",
       "\n",
       "[11162 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output dataframe\n",
    "print(\"Building output\")\n",
    "df_out = build_output_df(phenoss_computation, hpo_db, args.gene_conversion)\n",
    "df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c89a3f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "df_out.to_csv(args.outputfile, sep=\"\\t\", index=False)\n",
    "print(\"Saved results to\", args.outputfile)\n",
    "\n",
    "# Optional gene subset (keeps your “LMNA” style check without hard-coded paths)\n",
    "if args.gene_of_interest is not None:\n",
    "    print(\"Found the gene of interest. Finding its ranking...\")\n",
    "    x = df_out[df_out[\"gene\"] == args.gene_of_interest].reset_index()\n",
    "    print(x.head())\n",
    "    if args.gene_outfile is not None:\n",
    "        x.to_csv(args.gene_outfile, sep=\"\\t\", index=False)\n",
    "        print(\"Saved gene subset to\", args.gene_outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phenogpt2",
   "language": "python",
   "name": "phenogpt2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
